import requests
from bs4 import BeautifulSoup
import pandas as pd
import logging
from time import sleep
from random import uniform
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

# ìš”ì²­ í•¨ìˆ˜ (ê°„ë‹¨í•œ ì¬ì‹œë„ í¬í•¨)
def fetch(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    for attempt in range(3):
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            resp.raise_for_status()
            return resp
        except Exception as e:
            logging.warning(f"Fetch failed ({attempt+1}/3): {url} -> {e}")
            sleep(2)
    return None

def extract_bill_title_text(soup):
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ì „ì²´ ë²•ì•ˆ ì œëª© í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ ê²¬ê³ í•˜ê²Œ ì¶”ì¶œ
    ìš°ì„ ìˆœìœ„: h3.detailh3 > h1.tit > p.bill_title
    """
    tag = soup.find("h3", class_="detailh3")
    if not tag:
        tag = soup.find("h1", class_="tit")
    if not tag:
        tag = soup.find("p", class_="bill_title")
    if not tag:
        return None
    # ë‚´ë¶€ ì¤„ë°”ê¿ˆ/ê³µë°± ì •ë¦¬
    return tag.get_text(" ", strip=True)

def parse_bill_title_for_ids_and_cosponsors(title_text):
    """
    ì œëª© ì˜ˆ:
    [2103147] ì‹í’ˆì•ˆì „ê¸°ë³¸ë²• ì¼ë¶€ê°œì •ë²•ë¥ ì•ˆ(ê°•ì„ ìš°ì˜ì› ë“± 10ì¸)
    [2111105] 6_25ì „ìŸ ... (í•œê¸°í˜¸ì˜ì› ë“± 13ì¸)

    ë°˜í™˜: (bill_num, cosponsor_count)
    - bill_num: ëŒ€ê´„í˜¸ ì•ˆ ìˆ«ì
    - cosponsor_count: 'ë“± xxì¸'ì˜ xx (int). ì—†ìœ¼ë©´ None
    """
    bill_num = None
    cosponsor_count = None

    if title_text:
        m_id = re.search(r'\[(\d+)\]', title_text)
        if m_id:
            bill_num = m_id.group(1)

        # 'ë“± xxì¸' íŒ¨í„´ (ê³µë°± í—ˆìš©)
        m_co = re.search(r'ë“±\s*([0-9]+)\s*ì¸', title_text)
        if m_co:
            try:
                cosponsor_count = int(m_co.group(1))
            except:
                cosponsor_count = None

    return bill_num, cosponsor_count

def parse_cosponsors_from_page(bill_id: str):
    """
    bill_id ìƒì„¸ í˜ì´ì§€ ì ‘ì† â†’ ì œëª© í…ìŠ¤íŠ¸ í™•ë³´ â†’ bill_num/ê³µë™ë°œì˜ì ìˆ˜ ì¶”ì¶œ
    """
    url = f"https://likms.assembly.go.kr/bill/bi/billDetailPage.do?billId={bill_id}"
    resp = fetch(url)
    if not resp:
        return None, None, None

    resp.encoding = "utf-8"
    soup = BeautifulSoup(resp.text, "html.parser")

    title_text = extract_bill_title_text(soup)
    bill_num, cosponsor_count = parse_bill_title_for_ids_and_cosponsors(title_text)

    # ì¶”ê°€ ì•ˆì „ì¥ì¹˜: ì œëª© íƒœê·¸ë¥¼ ëª» ì°¾ì•˜ì„ ë•Œ í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í•œ ë²ˆ ë” ì‹œë„
    if title_text is None:
        page_text = soup.get_text(" ", strip=True)
        bill_num2, cosponsor_count2 = parse_bill_title_for_ids_and_cosponsors(page_text)
        bill_num = bill_num or bill_num2
        cosponsor_count = cosponsor_count if cosponsor_count is not None else cosponsor_count2
        title_text = page_text  # ë””ë²„ê¹… í™•ì¸ìš©

    return bill_num, cosponsor_count, title_text

if __name__ == "__main__":
    # ğŸ“‚ bill_id.txt ë¡œë“œ
    with open("bill_id.txt", "r", encoding="utf-8") as f:
        bill_ids = [line.strip() for line in f if line.strip()]

    logging.info(f"Loaded {len(bill_ids)} bill IDs")

    rows = []
    for i, bill_id in enumerate(bill_ids, 1):
        bill_num, cos_cnt, title_text = parse_cosponsors_from_page(bill_id)

        rows.append({
            "orig_bill_id": bill_id,        # ì›ë˜ PRC... ID
            "bill_num": bill_num,           # [xxxxxxx] ìˆ«ì
            "cosponsor_count": cos_cnt,     # ê³µë™ë°œì˜ì ìˆ˜ (ì—†ìœ¼ë©´ None)
            "title_text": title_text        # (ì„ íƒ) ì›ì œëª©: ì¶”ì¶œ ì‹¤íŒ¨ì‹œ ë””ë²„ê¹…ìš©
        })

        logging.info(f"[{i}/{len(bill_ids)}] {bill_id} -> bill_num={bill_num}, cosponsors={cos_cnt}")
        sleep(uniform(0.8, 1.5))  # ì„œë²„ ë¶€ë‹´ ë°©ì§€

    # CSV ì €ì¥ (ì›í•˜ì‹œë©´ title_text ì»¬ëŸ¼ì€ ì œê±° ê°€ëŠ¥)
    df = pd.DataFrame(rows)
    df.to_csv("bill_cosponsors.csv", index=False, encoding="utf-8-sig")
    logging.info("âœ… Saved as bill_cosponsors.csv")
